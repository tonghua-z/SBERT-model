{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR2275tF25VX",
        "outputId": "66a4183e-1bc4-466b-85a5-19a9398d1c4f"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.1 MB 12.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asa0qZek6a8s"
      },
      "source": [
        "## SentenceBERT for NLI objective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er1LedGaAAUh"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
        "\n",
        "class SBERT(nn.Module):\n",
        "    def __init__(self, config, dimension, device, max_length):\n",
        "        super(SBERT, self).__init__()\n",
        "        self.config = config\n",
        "        self.dim = dimension\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize the model\n",
        "        self.config = AutoConfig.from_pretrained(config)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config)\n",
        "        self.bert_net = AutoModel.from_pretrained(config).to(self.device)\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.dim * 3, 3),\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, sentence1, sentence2, pooling=\"mean\"):\n",
        "        # Encode the first sentence \n",
        "        seq_indexed_1 = self.tokenizer(sentence1, padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        input_ids_1 = torch.tensor(seq_indexed_1['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        att_mask_1 = torch.tensor(seq_indexed_1['attention_mask'], dtype=torch.int64).to(self.device)\n",
        "        embedding1 = self.bert_net(input_ids_1, attention_mask=att_mask_1)[0]\n",
        "\n",
        "        # Encode the second sentence \n",
        "        seq_indexed_2 = self.tokenizer(sentence2, padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        input_ids_2 = torch.tensor(seq_indexed_2['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        att_mask_2 = torch.tensor(seq_indexed_2['attention_mask'], dtype=torch.int64).to(self.device)\n",
        "        embedding2 = self.bert_net(input_ids_2, attention_mask=att_mask_2)[0]\n",
        "        \n",
        "        # Pooling layer\n",
        "        if pooling == \"mean\":\n",
        "          u = self.mean_pooling_strategy(embedding1, att_mask_1)\n",
        "          v = self.mean_pooling_strategy(embedding2, att_mask_2)\n",
        "          \n",
        "        elif pooling == \"max\":\n",
        "          u = self.max_pooling_strategy(embedding1, att_mask_1)\n",
        "          v = self.max_pooling_strategy(embedding2, att_mask_2)\n",
        "\n",
        "        elif pooling == \"cls\":\n",
        "          u = self.cls_pooling_strategy(embedding1, att_mask_1)\n",
        "          v = self.cls_pooling_strategy(embedding2, att_mask_2)\n",
        "        else: \n",
        "          raise ValueError(\"Pooling should be mean, max or cls.\")\n",
        "\n",
        "        return self.output_layer(torch.cat((u, v, torch.abs(u - v)), 1)).to(self.device)\n",
        "\n",
        "    # Three pooling strategies\n",
        "\n",
        "    def mean_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      expanded_att_mask = att_mask.unsqueeze(-1).expand(sentence_embedding.size()).float()\n",
        "      sum_embedding = (sentence_embedding * expanded_att_mask).sum(1)\n",
        "      sum_att_mask = torch.clamp(expanded_att_mask.sum(1), min=1e-9)\n",
        "      return sum_embedding / sum_att_mask\n",
        "\n",
        "    def max_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      expanded_att_mask = att_mask.unsqueeze(-1).expand(sentence_embedding.size()).float()\n",
        "      sentence_embedding[expanded_att_mask == 0] = -1e9\n",
        "      return torch.max(sentence_embedding, 1)[0]\n",
        "\n",
        "    def cls_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      return sentence_embedding[:,0]\n",
        "\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mRlEGfS4QJV"
      },
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "np.random.seed(233)\n",
        "\n",
        "def load_snli_data(path, max_num=None):\n",
        "  sentence1, sentence2, labels = [], [], []\n",
        "  label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
        "  invalid_row = 0\n",
        "\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      row = line.strip().split('\\t')\n",
        "      if row[0] == 'gold_label':\n",
        "        continue\n",
        "      try:\n",
        "        label_id = label2int[row[0]]\n",
        "        labels.append(label_id)\n",
        "        sentence1.append(row[5])\n",
        "        sentence2.append(row[6])\n",
        "      except:\n",
        "        invalid_row += 1\n",
        "      if max_num!=None and len(labels)>=max_num:\n",
        "        break\n",
        "\n",
        "    random.shuffle(sentence1)\n",
        "    random.shuffle(sentence2)\n",
        "    random.shuffle(labels)\n",
        "\n",
        "    return sentence1, sentence2, labels, invalid_row\n",
        "\n",
        "def train(model_config, dim, batch_size, learning_rate, max_length, max_num, loss_function, pooling, device, train_data_path, model_save_path):\n",
        "\n",
        "    # Load and pre-process the training dataset\n",
        "    sentence1, sentence2, labels, invalid_row = load_snli_data(train_data_path, max_num=max_num)\n",
        "    print('Data Scale:', len(labels))\n",
        "    print('Invalid Row:', invalid_row)\n",
        "\n",
        "    # Initialization\n",
        "    model = SBERT(config, dim, device, max_length)\n",
        "    model.to(device)\n",
        "    train_loss = loss_function\n",
        "    train_loss.to(device)\n",
        "    optimizer = Adam(params=[{'params': model.bert_net.parameters(), 'lr': learning_rate},\n",
        "                  {'params': model.output_layer.parameters(), 'lr': learning_rate},], lr=learning_rate)\n",
        "    \n",
        "    # Warm up + Cosine Anneal\n",
        "    warm_up_iter = len(labels)//10\n",
        "    t_max = 50\n",
        "    lr_max = 0.1\t\n",
        "    lr_min = 1e-5\t\n",
        "\n",
        "    lambda0 = lambda cur_iter: 1\n",
        "    lambda1 = lambda cur_iter: cur_iter / warm_up_iter if  cur_iter < warm_up_iter else \\\n",
        "            (lr_min + 0.5*(lr_max-lr_min)*(1.0+math.cos( (cur_iter-warm_up_iter)/(t_max-warm_up_iter)*math.pi)))/0.1\n",
        "\n",
        "    WarmUp_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda0, lambda1])\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    # Training the model\n",
        "    \n",
        "    for i in range(0, len(labels), batch_size):\n",
        "        if i + batch_size <= len(labels):\n",
        "          features = model(sentence1[i: i+batch_size], sentence2[i: i+batch_size], pooling).to(device)\n",
        "          loss = train_loss(features, torch.tensor(labels[i: i+batch_size], dtype=torch.int64).to(device)).to(device)\n",
        "        else:\n",
        "          features = model(sentence1[i:], sentence2[i:], pooling).to(device)\n",
        "          loss = train_loss(features, torch.tensor(labels[i:], dtype=torch.int64).to(device)).to(device)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        WarmUp_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        print('batch:{0}    loss:{1}'.format(i//batch_size, round(float(loss),5)))\n",
        "    torch.save(model.state_dict(), model_save_path)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SobrQUk9Iym7",
        "outputId": "29b6f8c3-653b-48f3-e6e8-8190c9131f68"
      },
      "source": [
        "config = 'bert-base-uncased'\n",
        "dim = 768\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "max_length = 30\n",
        "max_num = 1000\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "pooling = 'mean'\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "train_data_path = '/content/drive/MyDrive/sbert_data/snli_1.0_train.txt'\n",
        "model_save_path = 'SentenceBERT_SNLI_1t'\n",
        "\n",
        "train( config, \n",
        "    dim, \n",
        "    batch_size, \n",
        "    learning_rate, \n",
        "    max_length, \n",
        "    max_num,\n",
        "    loss_function,\n",
        "    pooling, \n",
        "    device, \n",
        "    train_data_path, \n",
        "    model_save_path)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Scale: 1000\n",
            "Invalid Row: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch:0    loss:1.142\n",
            "batch:1    loss:1.11523\n",
            "batch:2    loss:1.03957\n",
            "batch:3    loss:1.09322\n",
            "batch:4    loss:1.12638\n",
            "batch:5    loss:1.09815\n",
            "batch:6    loss:1.13145\n",
            "batch:7    loss:1.08921\n",
            "batch:8    loss:1.15764\n",
            "batch:9    loss:1.06695\n",
            "batch:10    loss:1.20151\n",
            "batch:11    loss:1.09614\n",
            "batch:12    loss:1.11692\n",
            "batch:13    loss:1.13099\n",
            "batch:14    loss:1.1068\n",
            "batch:15    loss:1.11822\n",
            "batch:16    loss:1.11792\n",
            "batch:17    loss:1.1141\n",
            "batch:18    loss:1.06325\n",
            "batch:19    loss:1.07677\n",
            "batch:20    loss:1.14935\n",
            "batch:21    loss:1.07231\n",
            "batch:22    loss:1.13179\n",
            "batch:23    loss:1.05522\n",
            "batch:24    loss:1.1208\n",
            "batch:25    loss:1.06678\n",
            "batch:26    loss:1.13989\n",
            "batch:27    loss:1.11314\n",
            "batch:28    loss:1.11799\n",
            "batch:29    loss:1.11564\n",
            "batch:30    loss:1.07944\n",
            "batch:31    loss:1.16056\n",
            "batch:32    loss:1.10456\n",
            "batch:33    loss:1.10476\n",
            "batch:34    loss:1.08606\n",
            "batch:35    loss:1.12279\n",
            "batch:36    loss:1.10558\n",
            "batch:37    loss:1.09366\n",
            "batch:38    loss:1.11663\n",
            "batch:39    loss:1.07725\n",
            "batch:40    loss:1.1347\n",
            "batch:41    loss:1.12857\n",
            "batch:42    loss:1.06179\n",
            "batch:43    loss:1.13902\n",
            "batch:44    loss:1.15489\n",
            "batch:45    loss:1.1466\n",
            "batch:46    loss:1.20369\n",
            "batch:47    loss:1.13335\n",
            "batch:48    loss:1.09934\n",
            "batch:49    loss:1.14609\n",
            "batch:50    loss:1.1131\n",
            "batch:51    loss:1.09805\n",
            "batch:52    loss:1.14925\n",
            "batch:53    loss:1.10752\n",
            "batch:54    loss:1.05393\n",
            "batch:55    loss:1.17309\n",
            "batch:56    loss:1.13597\n",
            "batch:57    loss:1.11521\n",
            "batch:58    loss:1.09191\n",
            "batch:59    loss:1.09882\n",
            "batch:60    loss:1.06107\n",
            "batch:61    loss:1.0983\n",
            "batch:62    loss:1.06173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKr0VDL60B9j"
      },
      "source": [
        "## Extra Section Ⅰ: Evaluation of SentenceBert NLI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXeuRGLehAzu"
      },
      "source": [
        "def evaluator(model_config, dim, device, max_length, batch_size, sample_num, test_data_path, model_load_path):\n",
        "\n",
        "    sentence1, sentence2, labels, invalid_row = load_snli_data(test_data_path, max_num=sample_num)\n",
        "    print('Predicted Samples:', len(labels))\n",
        "    print('Invalid Row:', invalid_row)\n",
        "    model = SBERT(config, dim, device, max_length)\n",
        "    model.to(device)\n",
        "    parameters = torch.load(model_load_path)\n",
        "    model.load_state_dict(parameters)\n",
        "    model.eval()\n",
        "\n",
        "    predicted_labels = []\n",
        "    for i in range(0, len(labels), batch_size):\n",
        "      if i + batch_size <= len(labels):\n",
        "          prediction = model(sentence1[i: i + batch_size], sentence2[i: i + batch_size]).to(device)\n",
        "      else:\n",
        "          prediction = model(sentence1[i:], sentence2[i:]).to(device)\n",
        "\n",
        "      predicted_labels += prediction.max(1)[1].tolist()\n",
        "\n",
        "    accuracy = 0\n",
        "    for i, j in zip(predicted_labels, labels):\n",
        "      if i == j:\n",
        "        accuracy += 1\n",
        "    print('Accuracy Rate:', accuracy/len(labels))\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L__q_ikrlZH9",
        "outputId": "4b556bab-d61e-4262-8885-01370bb31910"
      },
      "source": [
        "config = 'bert-base-uncased'\n",
        "dim = 768\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "max_length = 30\n",
        "batch_size = 16\n",
        "sample_num = 100\n",
        "test_data_path = '/content/drive/MyDrive/sbert_data/snli_1.0_test.txt'\n",
        "model_load_path = '/content/SentenceBERT_SNLI_1t'\n",
        "\n",
        "evaluator( config, \n",
        "      dim,\n",
        "      device, \n",
        "      max_length, \n",
        "      batch_size,\n",
        "      sample_num, \n",
        "      test_data_path, \n",
        "      model_load_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Samples: 100\n",
            "Invalid Row: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Rate: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ScmOWWhe36p"
      },
      "source": [
        "## Extra Section Ⅱ: SentenceBert model for STS(cosine similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZUx7ajedBsw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
        "\n",
        "class SBERT_STS(nn.Module):\n",
        "    def __init__(self, config, dimension, device, max_length):\n",
        "        super(SBERT_STS, self).__init__()\n",
        "        self.config = config\n",
        "        self.dim = dimension\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize the model\n",
        "        self.config = AutoConfig.from_pretrained(config)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config)\n",
        "        self.bert_net = AutoModel.from_pretrained(config).to(self.device)\n",
        "       \n",
        "\n",
        "    def forward(self, sentence1, sentence2, pooling=\"mean\"):\n",
        "        # Encode the first sentence \n",
        "        seq_indexed_1 = self.tokenizer(sentence1, padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        input_ids_1 = torch.tensor(seq_indexed_1['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        att_mask_1 = torch.tensor(seq_indexed_1['attention_mask'], dtype=torch.int64).to(self.device)\n",
        "        embedding1 = self.bert_net(input_ids_1, attention_mask=att_mask_1)[0]\n",
        "\n",
        "        # Encode the second sentence \n",
        "        seq_indexed_2 = self.tokenizer(sentence2, padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        input_ids_2 = torch.tensor(seq_indexed_2['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        att_mask_2 = torch.tensor(seq_indexed_2['attention_mask'], dtype=torch.int64).to(self.device)\n",
        "        embedding2 = self.bert_net(input_ids_2, attention_mask=att_mask_2)[0]\n",
        "        \n",
        "        # Pooling layer\n",
        "        if pooling == \"mean\":\n",
        "          u = self.mean_pooling_strategy(embedding1, att_mask_1)\n",
        "          v = self.mean_pooling_strategy(embedding2, att_mask_2)\n",
        "          \n",
        "        elif pooling == \"max\":\n",
        "          u = self.max_pooling_strategy(embedding1, att_mask_1)\n",
        "          v = self.max_pooling_strategy(embedding2, att_mask_2)\n",
        "\n",
        "        elif pooling == \"cls\":\n",
        "          u = self.cls_pooling_strategy(embedding1, att_mask_1)\n",
        "          v = self.cls_pooling_strategy(embedding2, att_mask_2)\n",
        "        else: \n",
        "          raise ValueError(\"Pooling should be mean, max or cls.\")\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        return (u * v).sum(1) / torch.sqrt( pow(u,2) + pow(v,2) ).sum(1)\n",
        "\n",
        "    # Three pooling strategies\n",
        "\n",
        "    def mean_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      expanded_att_mask = att_mask.unsqueeze(-1).expand(sentence_embeddings.size()).float()\n",
        "      sum_embedding = (sentence_embedding * expanded_att_mask).sum(1)\n",
        "      sum_att_mask = torch.clamp(expanded_att_mask.sum(1), min=1e-9)\n",
        "      return sum_embedding / sum_att_mask\n",
        "\n",
        "    def max_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      expanded_att_mask = att_mask.unsqueeze(-1).expand(sentence_embedding.size()).float()\n",
        "      sentence_embedding[expanded_att_mask == 0] = -1e9\n",
        "      return torch.max(sentence_embedding, 1)[0]\n",
        "\n",
        "    def cls_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      return sentence_embedding[:,0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlGrwmnIFe-s"
      },
      "source": [
        "## Extra Section Ⅲ: SentenceBert model for triplet objective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6pxEJzO-r8S"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
        "\n",
        "class SBERT_TRI(nn.Module):\n",
        "    def __init__(self, config, dimension, device, max_length):\n",
        "        super(SBERT_TRI, self).__init__()\n",
        "        self.config = config\n",
        "        self.dim = dimension\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize the model\n",
        "        self.config = AutoConfig.from_pretrained(config)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config)\n",
        "        self.bert_net = AutoModel.from_pretrained(config).to(self.device)\n",
        "       \n",
        "\n",
        "    def forward(self, sentence1, sentence2, sentence3, pooling=\"mean\"):\n",
        "        # Encode the first sentence \n",
        "        seq_indexed_1 = self.tokenizer(sentence1, padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        input_ids_1 = torch.tensor(seq_indexed_1['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        att_mask_1 = torch.tensor(seq_indexed_1['attention_mask'], dtype=torch.int64).to(self.device)\n",
        "        embedding1 = self.bert_net(input_ids_1, attention_mask=att_mask_1)[0]\n",
        "\n",
        "        # Encode the second sentence \n",
        "        seq_indexed_2 = self.tokenizer(sentence2, padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        input_ids_2 = torch.tensor(seq_indexed_2['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        att_mask_2 = torch.tensor(seq_indexed_2['attention_mask'], dtype=torch.int64).to(self.device)\n",
        "        embedding2 = self.bert_net(input_ids_2, attention_mask=att_mask_2)[0]\n",
        "\n",
        "        # Encode the third sentence\n",
        "        seq_indexed_3 = self.tokenizer(sentence3, padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        input_ids_3 = torch.tensor(seq_indexed_3['input_ids'], dtype=torch.int64).to(self.device)\n",
        "        att_mask_3 = torch.tensor(seq_indexed_3['attention_mask'], dtype=torch.int64).to(self.device)\n",
        "        embedding3 = self.bert_net(input_ids_3, attention_mask=att_mask_3)[0]\n",
        "        \n",
        "        # Pooling layer\n",
        "        if pooling == \"mean\":\n",
        "          anchor_sentence = self.mean_pooling_strategy(embedding1, att_mask_1)\n",
        "          pos_sentence = self.mean_pooling_strategy(embedding2, att_mask_2)\n",
        "          neg_sentence = self.mean_pooling_strategy(embedding3, att_mask_3)\n",
        "          \n",
        "        elif pooling == \"max\":\n",
        "          anchor_sentence = self.max_pooling_strategy(embedding1, att_mask_1)\n",
        "          pos_sentence = self.max_pooling_strategy(embedding2, att_mask_2)\n",
        "          neg_sentence = self.max_pooling_strategy(embedding3, att_mask_3)\n",
        "         \n",
        "        elif pooling == \"cls\":\n",
        "          anchor_sentence = self.cls_pooling_strategy(embedding1, att_mask_1)\n",
        "          pos_sentence = self.cls_pooling_strategy(embedding2, att_mask_2)\n",
        "          neg_sentence = self.cls_pooling_strategy(embedding3, att_mask_3)\n",
        "          \n",
        "        else: \n",
        "          raise ValueError(\"Pooling should be mean, max or cls.\")\n",
        "          \n",
        "        return anchor_sentence, pos_sentence, neg_sentence\n",
        "\n",
        "    # Three pooling strategies\n",
        "\n",
        "    def mean_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      expanded_att_mask = att_mask.unsqueeze(-1).expand(sentence_embeddings.size()).float()\n",
        "      sum_embedding = (sentence_embedding * expanded_att_mask).sum(1)\n",
        "      sum_att_mask = torch.clamp(expanded_att_mask.sum(1), min=1e-9)\n",
        "      return sum_embedding / sum_att_mask\n",
        "\n",
        "    def max_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      expanded_att_mask = att_mask.unsqueeze(-1).expand(sentence_embedding.size()).float()\n",
        "      sentence_embedding[expanded_att_mask == 0] = -1e9\n",
        "      return torch.max(sentence_embedding, 1)[0]\n",
        "\n",
        "    def cls_pooling_strategy(self, sentence_embedding, att_mask):\n",
        "      return sentence_embedding[:,0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Oedd_UMBFYz"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Triplet loss function\n",
        "class Triplet_Loss(nn.Module):\n",
        "    def __init__(self, distance_metric= 'EUCLIDEAN' ,triplet_margin=1):\n",
        "        super(Triplet_Loss, self).__init__()\n",
        "        self.distance_metric = distance_metric\n",
        "        self.triplet_margin = triplet_margin\n",
        "    def forward(self, anchor_sentence, pos_sentence, neg_sentence):\n",
        "        if distance_metric== 'EUCLIDEAN':\n",
        "          pos_distance = F.pairwise_distance(anchor_sentence, pos_sentence, p=2)\n",
        "          neg_distance = F.pairwise_distance(anchor_sentence, neg_sentence, p=2)\n",
        "        elif distance_metric== 'MANHATTAN':\n",
        "          pos_distance = F.pairwise_distance(anchor_sentence, pos_sentence, p=1)\n",
        "          neg_distance = F.pairwise_distance(anchor_sentence, neg_sentence, p=1)\n",
        "        elif distance_metric== 'COSINE':\n",
        "          pos_distance = 1 - F.cosine_similarity(anchor_sentence, pos_sentence)\n",
        "          neg_distance = 1 - F.cosine_similarity(anchor_sentence, neg_sentence)\n",
        "        else:\n",
        "          raise ValueError(\"The distance metric can not be\", distance_metric)\n",
        "\n",
        "        loss = F.relu(pos_distance - neg_distance + self.triplet_margin)\n",
        "        return loss.mean()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMJ3OD-wWHk6"
      },
      "source": [
        "## Extra Section Ⅳ: Contrastive loss for siamese network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xasc-nC4Wb3g"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Contrastive_Loss(nn.Module):\n",
        "\n",
        "    def __init__(self, distance_metric= 'EUCLIDEAN', margin = 1):\n",
        "        super(Contrastive_Loss, self).__init__()\n",
        "        self.distance_metric = distance_metric\n",
        "        self.margin = margin\n",
        "        \n",
        "    def forward(self, sentence1, sentence2, labels):\n",
        "        distance = F.pairwise_distance(sentence1, sentence2, p=2)\n",
        "        loss = 0.5 * (labels * distance.pow(2) + (1 - labels) * F.relu(self.margin - distance).pow(2))\n",
        "        return losses.mean()"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}